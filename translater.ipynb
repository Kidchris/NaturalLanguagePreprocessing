{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# #############################################################\n",
    "# # @Open A TSV file and store the content into a list of     #\n",
    "# #############################################################\n",
    "def open_tsv_return_list(filename= \"./English-French.tsv\", preview_length= 5):\n",
    "    \"\"\"\n",
    "    Open a Tsv file and return a list -- \n",
    "    The return type is a list of strings mapping source language to target language\n",
    "    eg:(\"English --> French\")\n",
    "    Params : \n",
    "            filename = the filename of the file to open\n",
    "            preview_length = the length of the elements to see\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().replace(\"\\u202f\", \"\").lower().encode(\"utf-8\")\n",
    "\n",
    "    content = content.decode()\n",
    "    content = content.split(\"\\n\")\n",
    "    print(content[:preview_length])\n",
    "    return content"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# #########################################################\n",
    "# #@ Map the data and ignore the index of for each subset #\n",
    "# #########################################################\n",
    "def pair_value(content):\n",
    "    \"\"\"\n",
    "    Take the return value of tsv pair values and return \n",
    "    a list in the right format.\n",
    "    \"\"\"\n",
    "    trad = list()\n",
    "    for sentences in content:\n",
    "        pair = sentences.split(\"\\t\")\n",
    "        pair_content = list()\n",
    "        i = 1\n",
    "        for sent in pair:\n",
    "            if (i==1) or (i==3):\n",
    "                pass\n",
    "            else:\n",
    "                pair_content.append(sent)\n",
    "            i = i+1\n",
    "        trad.append(pair_content)\n",
    "    return trad\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# ################################# \n",
    "# # @Clean the dataset            #\n",
    "# #################################\n",
    "def clean_pair(mapper) :\n",
    "    sign = '!\"#$%&()*+,./:;<=>?@[\\\\]_`{|}~'\n",
    "    table = str.maketrans('', '', sign)\n",
    "\n",
    "    clean_doc = list()\n",
    "\n",
    "    for pair_sent in mapper:\n",
    "        sub_sentence = list()\n",
    "        for sentence in pair_sent:\n",
    "            sentence = sentence.split()\n",
    "            words = [w.translate(table) for w in sentence]\n",
    "            sub_sentence.append(\" \".join(words))\n",
    "        clean_doc.append(sub_sentence)\n",
    "    return clean_doc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#\n",
    "# # @Apply the above functions \n",
    "#\n",
    "content = open_tsv_return_list()\n",
    "translations = pair_value(content)\n",
    "clean_doc = clean_pair(translations)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"\\ufeff1276\\tlet's try something.\\t3091\\tessayons quelque chose!\", \"1276\\tlet's try something.\\t456963\\ttentons quelque chose!\", '1277\\ti have to go to sleep.\\t373908\\tje dois aller dormir.', \"1280\\ttoday is june 18th and it is muiriel's birthday!\\t3095\\taujourd'hui nous sommes le 18 juin et c'est l'anniversaire de muiriel!\", \"1280\\ttoday is june 18th and it is muiriel's birthday!\\t696081\\taujourd'hui c'est le 18 juin, et c'est l'anniversaire de muiriel.\"]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# #########################################################\n",
    "# # @Split the content into to English sentences onnly    #\n",
    "# #########################################################\n",
    "def split_to_eng_content(clean_doc):\n",
    "    english_sentences = []\n",
    "    for sentence_pair in clean_doc:\n",
    "        en = sentence_pair[0] if len(sentence_pair)>1 else \"\"\n",
    "        english_sentences.append(en)\n",
    "        \n",
    "    return english_sentences\n",
    "# #########################################################\n",
    "# # @Split the content into to French sentences onnly     #\n",
    "# #########################################################\n",
    "def split_to_fr_content(clean_doc):\n",
    "    french_sentences = []        \n",
    "    for sentence_pair in clean_doc:\n",
    "        fr = sentence_pair[-1] if len(sentence_pair)>1 else \"\"\n",
    "        french_sentences.append(fr)\n",
    "        \n",
    "    return french_sentences\n",
    "\n",
    "# ##### @Apply the Functions\n",
    "english_sentences = split_to_eng_content(clean_doc)\n",
    "french_sentences = split_to_fr_content(clean_doc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# ###################################################\n",
    "#   # @Save content to English text only -->en_file #\n",
    "# ###################################################\n",
    "def sentences_saver_to_eng(en_file= \"english_sentences.txt\",\n",
    "                        english_sentences= split_to_eng_content(clean_doc)):\n",
    "    file = open(en_file, \"w\")\n",
    "    for line in english_sentences:\n",
    "        file.write(line+\"\\n\") \n",
    "    file.close()\n",
    "    return file.closed\n",
    "#####################################################   \n",
    "# # @Save content to French text only -->fr_file    #\n",
    "#####################################################\n",
    "def sentences_saver_to_fr(fr_file= \"french_sentences.txt\",\n",
    "                       french_sentences= split_to_eng_content(clean_doc) ):\n",
    "    file = open(fr_file, \"w\")\n",
    "    for line in french_sentences:\n",
    "        file.write(line+\"\\n\")\n",
    "    file.close()\n",
    "    return file.closed\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#\n",
    "# # @Split the dataset into 80% for training, 10% for test and 10% for validation\n",
    "#\n",
    "np.random.shuffle(clean_doc)\n",
    "dataset = clean_doc\n",
    "train_dataset                = dataset[:(len(dataset)*80)//100] \n",
    "test_validation_dataset      = dataset[(len(dataset)*80)//100:]\n",
    "test_dataset                 = test_validation_dataset[:(len(test_validation_dataset)*50)//100] \n",
    "validation_dataset           = test_validation_dataset[(len(test_validation_dataset)*50)//100:]\n",
    "del dataset\n",
    "del test_validation_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# ###########################\n",
    "# # @Create a tokenizer     #\n",
    "# ###########################\n",
    "def create_tokenizer(senteces_list):\n",
    "    tknz = Tokenizer()\n",
    "    tknz.fit_on_texts(senteces_list)\n",
    "    return tknz\n",
    "\n",
    "#\n",
    "# # @Find the longest sequence in the list\n",
    "#\n",
    "\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# ######################################\n",
    "#   #####  English Part Now #####      #\n",
    "# ######################################\n",
    "eng_tokenizer   = create_tokenizer(english_sentences)\n",
    "eng_max_seq     = max_length(english_sentences)\n",
    "eng_vocab_size  = len(eng_tokenizer.word_index)+1\n",
    "print(f\"The english vocab size is {eng_vocab_size}\")\n",
    "print(f\"The english longest sentence is of size: {eng_max_seq}\")\n",
    "# ######################################\n",
    "#   #####  French Part Now #####       #\n",
    "# ######################################\n",
    "fr_tokenizer    = create_tokenizer(french_sentences)\n",
    "fr_max_seq      = max_length(french_sentences)\n",
    "fr_vocab_size   = len(fr_tokenizer.word_index)+1\n",
    "print(f\"The french vocab size is {fr_vocab_size}\")\n",
    "print(f\"The french longest sentence is of size: {fr_max_seq}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The english vocab size is 30421\n",
      "The english longest sentence is of size: 262\n",
      "The french vocab size is 47506\n",
      "The french longest sentence is of size: 237\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "######################################\n",
    "# # @Feature Engeniering             #\n",
    "# ####################################\n",
    "def encoded_sequences(sentences_list, tokenizer, length):\n",
    "    encoded = tokenizer.texts_to_sequences(sentences_list)\n",
    "    encoded = pad_sequences(encoded, maxlen=length)\n",
    "    return encoded\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "fr_encoded_sequences = encoded_sequences(french_sentences, fr_tokenizer, fr_max_seq)\n",
    "eng_encoded_sequences = encoded_sequences(english_sentences, eng_tokenizer, eng_max_seq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "eng_encoded_sequences[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 171,\n",
       "       244, 111], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "61f16b98c194273433d28da8ad4eebd31a866a26d2fd43b3ba83f196243e57b5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}