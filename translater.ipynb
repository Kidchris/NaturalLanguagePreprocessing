{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 11:11:28.724335: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-20 11:11:28.724416: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################\n",
    "# # @Open A TSV file and store the content into a list of     #\n",
    "# #############################################################\n",
    "def open_tsv_return_list(filename : str = \"./English-French.tsv\"):\n",
    "    \"\"\"\n",
    "    Open a Tsv file and return a list -- \n",
    "    The return type is a list of strings mapping source language to target language\n",
    "    eg:(\"English --> French\")\n",
    "    Params : \n",
    "            filename = the filename of the file to open\n",
    "            preview_length = the length of the elements to see\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().replace(\"\\u202f\", \"\").lower().encode(\"utf-8\")\n",
    "\n",
    "    content = content.decode().split(\"\\n\")\n",
    "    print(\"Lenght of the dataset is : \", len(content))\n",
    "    return content[-10000:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# #@ Map the data and ignore the index of for each subset #\n",
    "# #########################################################\n",
    "def pair_value(content):\n",
    "    \"\"\"\n",
    "    Take the return value of tsv pair values and return \n",
    "    a list in the right format.\n",
    "    \"\"\"\n",
    "    trad = list()\n",
    "    for sentences in content:\n",
    "        pair = sentences.split(\"\\t\")\n",
    "        pair_content = list()\n",
    "        i = 1\n",
    "        for sent in pair:\n",
    "            if (i==1) or (i==3):\n",
    "                pass\n",
    "            else:\n",
    "                pair_content.append(sent)\n",
    "            i = i+1\n",
    "        trad.append(pair_content)\n",
    "    return trad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################# \n",
    "# # @Clean the dataset            #\n",
    "# #################################\n",
    "def clean_pair(mapper) :\n",
    "    sign = '!\"#$%&()*+,./:;<=>?@[\\\\]_`{|}~'\n",
    "    table = str.maketrans('', '', sign)\n",
    "\n",
    "    clean_doc = list()\n",
    "\n",
    "    for pair_sent in mapper:\n",
    "        sub_sentence = list()\n",
    "        for sentence in pair_sent:\n",
    "            sentence = sentence.split()\n",
    "            words = [w.translate(table) for w in sentence]\n",
    "            sub_sentence.append(\" \".join(words))\n",
    "        clean_doc.append(sub_sentence)\n",
    "    \n",
    "    del sign, table\n",
    "    return clean_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of the dataset is :  310290\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# # @Apply the above functions \n",
    "#\n",
    "content = open_tsv_return_list()\n",
    "translations = pair_value(content)\n",
    "clean_doc = clean_pair(translations)\n",
    "del content\n",
    "del translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to train and Test and validation and just Play with the trainnig dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# # @Split the dataset into 80% for training, 10% for test and 10% for validation\n",
    "#\n",
    "np.random.shuffle(clean_doc)\n",
    "train_dataset                = clean_doc[:(len(clean_doc)*80)//100] \n",
    "test_validation_dataset      = clean_doc[(len(clean_doc)*80)//100:]\n",
    "test_dataset                 = test_validation_dataset[:(len(test_validation_dataset)*50)//100] \n",
    "validation_dataset           = test_validation_dataset[(len(test_validation_dataset)*50)//100:]\n",
    "del test_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# # @Split the content into to English sentences onnly    #\n",
    "# #########################################################\n",
    "def split_to_eng_content(clean_doc):\n",
    "    english_sentences = []\n",
    "    for sentence_pair in clean_doc:\n",
    "        en = sentence_pair[0] if len(sentence_pair)>1 else \"\"\n",
    "        english_sentences.append(en)\n",
    "        \n",
    "    return english_sentences\n",
    "# #########################################################\n",
    "# # @Split the content into to French sentences onnly     #\n",
    "# #########################################################\n",
    "def split_to_fr_content(clean_doc):\n",
    "    french_sentences = []        \n",
    "    for sentence_pair in clean_doc:\n",
    "        fr = sentence_pair[-1] if len(sentence_pair)>1 else \"\"\n",
    "        french_sentences.append(fr)\n",
    "        \n",
    "    return french_sentences\n",
    "\n",
    "# ######## @Apply the Functions\n",
    "english_sentences = split_to_eng_content(train_dataset)\n",
    "french_sentences = split_to_fr_content(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################################\n",
    "#   # @Save content to English text only -->en_file #\n",
    "# ###################################################\n",
    "# def sentences_saver_to_eng(en_file= \"english_train_sentences.txt\",\n",
    "#                         english_sentences= split_to_eng_content(clean_doc)):\n",
    "#     file = open(en_file, \"w\")\n",
    "#     for line in english_sentences:\n",
    "#         file.write(line+\"\\n\") \n",
    "#     file.close()\n",
    "#     return file.closed\n",
    "# #####################################################   \n",
    "# # # @Save content to French text only -->fr_file    #\n",
    "# #####################################################\n",
    "# def sentences_saver_to_fr(fr_file= \"french_train_sentences.txt\",\n",
    "#                        french_sentences= split_to_eng_content(clean_doc) ):\n",
    "#     file = open(fr_file, \"w\")\n",
    "#     for line in french_sentences:\n",
    "#         file.write(line+\"\\n\")\n",
    "#     file.close()\n",
    "#     return file.closed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################\n",
    "# # @Create a tokenizer     #\n",
    "# ###########################\n",
    "def create_tokenizer(sentences_list):\n",
    "    tknz = Tokenizer()\n",
    "    tknz.fit_on_texts(sentences_list)\n",
    "    return tknz\n",
    "\n",
    "#\n",
    "# # @Find the longest sequence in the list\n",
    "#\n",
    "\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The english vocab size is 6750\n",
      "The english longest sentence is of size: 76\n",
      "The french vocab size is 8607\n",
      "The french longest sentence is of size: 81\n"
     ]
    }
   ],
   "source": [
    "# ######################################\n",
    "#   #####  English Part Now #####      #\n",
    "# ######################################\n",
    "eng_tokenizer   = create_tokenizer(english_sentences)\n",
    "eng_max_seq     = max_length(english_sentences)\n",
    "eng_vocab_size  = len(eng_tokenizer.word_index)+1\n",
    "print(f\"The english vocab size is {eng_vocab_size}\")\n",
    "print(f\"The english longest sentence is of size: {eng_max_seq}\")\n",
    "# ######################################\n",
    "#   #####  French Part Now #####       #\n",
    "# ######################################\n",
    "fr_tokenizer    = create_tokenizer(french_sentences)\n",
    "fr_max_seq      = max_length(french_sentences)\n",
    "fr_vocab_size   = len(fr_tokenizer.word_index)+1\n",
    "print(f\"The french vocab size is {fr_vocab_size}\")\n",
    "print(f\"The french longest sentence is of size: {fr_max_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# # @Feature Engeniering             #\n",
    "# ####################################\n",
    "def encoded_sequences(sentences_list, tokenizer, length):\n",
    "    encoded = tokenizer.texts_to_sequences(sentences_list)\n",
    "    encoded = pad_sequences(encoded, maxlen=length)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def encoded_output(sequences, vocab_size, tokenizer=fr_tokenizer):\n",
    "    y = []\n",
    "    for sequence in tokenizer.texts_to_sequences(sequences):\n",
    "        encode = to_categorical(sequence, num_classes=vocab_size)\n",
    "        y.append(encode)\n",
    "    print(np.array(y).shape)\n",
    "    # y = np.array(y).reshape(np.array(sequences).shape[0], np.array(sequences).shape[1], vocab_size)\n",
    "    # return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_encoded_sequences = encoded_output(french_sentences, fr_vocab_size)\n",
    "eng_encoded_sequences = encoded_sequences(english_sentences, eng_tokenizer, eng_max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/home/chris/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:90:327199)",
      "at w.execute (/home/chris/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:90:326520)",
      "at w.start (/home/chris/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:90:322336)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at t.CellExecutionQueue.executeQueuedCells (/home/chris/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:90:336863)",
      "at t.CellExecutionQueue.start (/home/chris/.vscode/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:90:336403)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model\n",
    " \n",
    "# define model\n",
    "model = define_model(fr_vocab_size, eng_vocab_size, fr_max_seq, eng_max_seq, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(eng_encoded_sequences, fr_encoded_sequences, epochs=30, \n",
    "          batch_size=128, validation_split=0.25, callbacks=[checkpoint], verbose=2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61f16b98c194273433d28da8ad4eebd31a866a26d2fd43b3ba83f196243e57b5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
