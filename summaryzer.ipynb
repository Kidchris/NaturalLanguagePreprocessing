{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 250)\n",
    "warnings.filterwarnings(\"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans(\"\", \"\", punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data\n",
    "- Remove Punctuation\n",
    "- Lower All the sentences\n",
    "- Remove Punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv(\"./summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_list, table=table) :\n",
    "    output_list = []\n",
    "    for row in input_list:\n",
    "        row_list = \"\"\n",
    "        for contract_k, contract_v in contraction_mapping.items():\n",
    "            row.replace(contract_k, contract_v)\n",
    "        for word in row.split():\n",
    "            # if word not in stop_words:\n",
    "            word = word.translate(table)\n",
    "            row_list += word.lower() + \" \"\n",
    "        output_list.append(row_list.strip())\n",
    "        \n",
    "    return np.array(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = preprocess(data.headlines.to_list())\n",
    "text = preprocess(data.text.to_list())\n",
    "\n",
    "data.headlines = headlines\n",
    "data.text = text\n",
    "data.headlines = data.headlines.apply(lambda x: \"_START \" + x + \" _END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"Review:\",data['text'][i])\n",
    "    print(\"Summary:\",data['headlines'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del headlines\n",
    "del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['headlines']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(tok_object, x):\n",
    "    all_ = x\n",
    "    tk = tok_object\n",
    "    tk.fit_on_texts(all_)\n",
    "    return tk\n",
    "\n",
    "x_tok = tokenizer(Tokenizer(), train.text.to_list())\n",
    "y_tok = tokenizer(Tokenizer(), train.headlines.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOkenize the train data\n",
    "def tokenize(input_, pad_x_size:int = 70, pad_y_size:int = 20, x_tok=x_tok, y_tok=y_tok):\n",
    "    \n",
    "    y = input_.headlines.to_numpy()\n",
    "    x = input_.text.to_numpy()\n",
    "    \n",
    "    headlines = y_tok.texts_to_sequences(np.array(y))\n",
    "    text = x_tok.texts_to_sequences(np.array(x))\n",
    "    \n",
    "    headlines = pad_sequences(headlines, padding=\"post\", maxlen=pad_y_size)\n",
    "    text = pad_sequences(text, padding=\"post\", maxlen=pad_x_size)\n",
    "    \n",
    "    return headlines, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_train, text_train = tokenize(train)\n",
    "headlines_test, text_test = tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(headlines_test) == len(text_test)\n",
    "assert len(headlines_train) == len(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The model and train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_voc_size  =   len(y_tok.word_index) +1\n",
    "x_voc_size  =   len(x_tok.word_index) +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import ( Embedding, LSTM, RepeatVector, TimeDistributed, \n",
    "                                    Dense, Input, Bidirectional, Concatenate)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from attention import AttentionLayer\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 256\n",
    "\n",
    "## Encoder With Lstm\n",
    "inputs = Input(shape=(70,))\n",
    "embedding = Embedding(x_voc_size, dim, trainable=True)(inputs)\n",
    "#LSTM1\n",
    "lstm1 = LSTM(dim, return_sequences=True, return_states=True)\n",
    "encoder_output1, state_h1, state_c1 = lstm1(embedding)\n",
    "# LSTM2\n",
    "lstm1 = LSTM(dim, return_sequences=True, return_states=True)\n",
    "encoder_output2, state_h2, state_c2 = lstm1(encoder_output1)\n",
    "# LSTM3\n",
    "lstm1 = LSTM(dim, return_sequences=True, return_states=True)\n",
    "encoder_outputs, state_h, state_c = lstm1(encoder_output2)\n",
    "\n",
    "# Decoder Inputs\n",
    "inputs_decoder = Input( shape=(None,))\n",
    "decoder_embedding_layer = Embedding(x_voc_size, dim, trainable=True)\n",
    "decoder_outputs_embedding = decoder_embedding_layer(inputs_decoder)\n",
    "# LSTM Decoder\n",
    "decoder_lstm = LSTM( dim, return_sequences=True, return_states=True)\n",
    "decoder_outputs, decoder_fwd_state, decoder_bwd_state = decoder_lstm(\n",
    "    decoder_outputs_embedding, initial_state=[state_h, state_c])\n",
    "#Attention, layer, \n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "# Concatenate the layers\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")[decoder_outputs, attn_out]\n",
    "# Dense Output layer\n",
    "dense_decoder = TimeDistributed(Dense(y_voc_size, activation=\"softmax\"))\n",
    "decoder_outputs = dense_decoder(decoder_concat_input)\n",
    "\n",
    "model = Model([inputs, inputs_decoder], decoder_outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer =\"rmsprop\")\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
    "checkpoint = ModelCheckpoint(\"./temp/model_best.h5\", save_best_only= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.array(text_train), np.array(headlines_train),validation_split=.25 ,epochs=128)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "509d05def373b697fdb010a6aa02fbb88284043a56880425e3dd344f44325f75"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
